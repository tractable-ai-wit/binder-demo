{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Tractable's ML Classifier Workshop!\n",
    "\n",
    "[Tractable linkedIn](https://www.linkedin.com/company/tractable/) | [Anifah](https://www.linkedin.com/in/anifah-bhadmus-891a7a116/) | [Martha](https://www.linkedin.com/in/martha-rose-robinson-14567a40/) | [Wei Ann](https://www.linkedin.com/in/weiann-heng/)\n",
    "\n",
    "Today, we will use industry-standard techniques to build a Machine Learning (ML) Classifier using supervised learning. \n",
    "\n",
    "We will use our classifier to classify vehicles by type (car, truck, SUV, van). Supervised learning means that we have test data that we can compare our results to. \n",
    "\n",
    "Start with: A dataset that gives us images (input) and labels (teaching signal/feedback). \n",
    "End with: An array of 4 numbers for each of those images (i.e. their classification confidences!)\n",
    "\n",
    "1. Create training, validation, and test data sets\n",
    "2. Build a data pipeline\n",
    "3. Explore a pre-trained model\n",
    "4. Perform feature extraction to get classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before we start\n",
    "1. Housekeeping \n",
    "2. How to use Jupyter notebook\n",
    "3. Quick concepts\n",
    "\n",
    "### Neural network\n",
    "Source: IBM\n",
    "\n",
    "<img src=\"https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_01_03-DeepNeuralNetwork-WHITEBG.png\" alt=\"neural-network\" width=\"500px\">\n",
    "\n",
    "### Feature \n",
    "An individual measureable property/characteristic. These are usually numeric, but we can think of some qualitative examples\n",
    "\n",
    "[Source](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.nosimpler.me%2Fmachine-learning%2F&psig=AOvVaw1i03lInhxU-kNY9Y2zn59a&ust=1619104334232000&source=images&cd=vfe&ved=0CAoQjRxqFwoTCJiZg-jPj_ACFQAAAAAdAAAAABAJ)\n",
    "<img src=\"https://www.nosimpler.me/wp-content/uploads/2016/08/ml-features-1.jpg\" alt=\"feature\" width=\"400px\">\n",
    "\n",
    "### Classifier\n",
    "A classifier is the thing that we train to classify data into classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from workshop_utils import extract_data, get_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create our datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: Create a training, validation, and test dataset from available data\n",
    "\n",
    "#### Input: \n",
    "Stanford Cars196 dataset (Dataset citation: 3D Object Representations for Fine-Grained Categorization. Jonathan Krause, Michael Stark, Jia Deng, Li Fei-Fei. 4th IEEE Workshop on 3D Representation and Recognition, at ICCV 2013 (3dRR-13). Sydney, Australia. Dec. 8, 2013.)\n",
    "\n",
    "TODO: more information about input: files and labels (0,1,2,3 which correspond to car, truck, SUV, van)\n",
    "\n",
    "\n",
    "#### Output:\n",
    "3 datasets (training, validation, test) that generate batches of images and class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract compressed dataseet\n",
    "extract_data()\n",
    "\n",
    "# Function to convert files into an matrix 2D images and labels.\n",
    "# Each 2D image is made out of 3 channels \n",
    "def load_data(path, label):\n",
    "    image = tf.image.decode_jpeg(tf.io.read_file(path), channels=3)\n",
    "    return image, label\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(get_data('train'))\n",
    "train_dataset = train_dataset.map(load_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's take a peep at this dataset using our load_data function\n",
    "fig, axarr = plt.subplots(2, 5, figsize=(10, 5))\n",
    "axes = axarr.flatten()\n",
    "for i, (image, label) in enumerate(train_dataset.take(10)):\n",
    "    ax = axes[i]\n",
    "    ax.imshow(image.numpy())\n",
    "    ax.set_title(int(label))\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 3 datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(get_data('train')).map(load_data)\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices(get_data('val')).map(load_data)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(get_data('test')).map(load_data)\n",
    "\n",
    "\n",
    "# Batch size = number of training exmaples utilized in one iteration. Important to not over/underfit\n",
    "batch_size = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.cache().batch(batch_size).prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.cache().batch(batch_size).prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.cache().batch(batch_size).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "image_batch, label_batch = next(iter(train_dataset))\n",
    "print(image_batch.shape)\n",
    "print(label_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: Create augmented data set to improve training\n",
    "\n",
    "#### Input: \n",
    "Training dataset from previous section\n",
    "\n",
    "#### Output:\n",
    "Larger training data set with augmentation?? (this isn't actually true - we are just showing what the augumented data looks like, discuss with team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using tensorflow library\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "# Link to library https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing\n",
    "# CODE-ALONG CHECKPOINT 1 - use tf.keras to pre-process images\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  preprocessing.?,\n",
    "  preprocessing.?,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_dataset.take(1):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    first_image = images[0]\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        augmented_image = data_augmentation(\n",
    "            tf.expand_dims(first_image, 0), training=True\n",
    "        )\n",
    "        plt.imshow(augmented_image[0].numpy().astype(\"int32\"))\n",
    "        plt.title(int(labels[0]))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How does a pre-trained model perform?\n",
    "\n",
    "### Goal: Test how good Google's pre-trained MobileNet V2 model is at classifying vehicles out of the box\n",
    "\n",
    "This is an example of how real world AI applications are built - use pre-trained model rather than building from scratch. Won't go into too much detail\n",
    "\n",
    "#### Input: \n",
    "1. Google's MobileNet V2 model, which is pre-trained on ImageNet dataset with a wide variety of categories (not just vehicles, but things like food, people, etc)\n",
    "2. Our training Cars196 dataset\n",
    "\n",
    "#### Output:\n",
    "\n",
    "The images of each vehicle and what MobileNet thinks this image is. (Remember that this model is general, so will not output 0,1,2,3 like you might expect.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (160, 160, 3)\n",
    "\n",
    "# Prepare data to suit MobileNet's expected input\n",
    "# Weights trained on imagenet\n",
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "mobilenet_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, weights='imagenet')\n",
    "\n",
    "# Create a tf.Keras model based on our inputs and MobileNet\n",
    "input_layer = tf.keras.Input(shape=IMG_SHAPE)\n",
    "# We don't use this next line anywhere else, will check with team and remove\n",
    "preprocessed_input_layer = preprocess_input(input_layer)\n",
    "mobilenet_layer = mobilenet_model(preprocessed_input_layer)\n",
    "model = tf.keras.Model(input_layer, mobilenet_layer)\n",
    "\n",
    "# Get model's predictions on our previously created image batch\n",
    "prediction_batch = model(image_batch)\n",
    "\n",
    "# Decode these predictions into human-readable class names (from imagenet)\n",
    "decoded_predictions = tf.keras.applications.mobilenet_v2.decode_predictions(prediction_batch.numpy(), top=1)\n",
    "\n",
    "fig, axarr = plt.subplots(8, 4, figsize=(10,20))\n",
    "axes = axarr.flatten()\n",
    "for i, (image, prediction) in enumerate(zip(image_batch, decoded_predictions)):\n",
    "    ax = axes[i]\n",
    "    ax.imshow(image.numpy())\n",
    "    top_label = prediction[0][1]\n",
    "    score = prediction[0][2]\n",
    "    ax.set_title(f'{top_label}, {score:.2f}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature extraction\n",
    "\n",
    "### Goal: Use an existing model to extract features from our images, and transfer that knowledge to our new task\n",
    "\n",
    "*Use mobilenet v2 to extract useful visual features and build a new classifier on top*\n",
    "\n",
    "### Step by step runthrough:\n",
    "1. Cut the top off the pre-trained model MobileNet V2 and use it to extract visual features\n",
    "2. Add a new classifier head on top that has the outputs we need for our task (car, truck, SUV, van)\n",
    "3. Connect this to our data pipeline to build a keras model ready for training\n",
    "\n",
    "#### Input: \n",
    "The Mobilenet V2 pre-trained model, and our data pipeline we constructed earlier.\n",
    "#### Output\n",
    "A new model architecture designed for our task, that leverages what the pretrained mobilenet has already learned.\n",
    "<img src=\"feature_extraction_methodology.jpg\" alt=\"Using an existing model as a feature extractor\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Cut the top off the pre-trained model MobileNet V2 and use it to extract visual features\n",
    "\n",
    "*Remove the \"top\" of the pre-trained model - meaning that it will not produce final classifications, but provide us insights about features of each datapoint. Make sure to freeze the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (160, 160, 3)\n",
    "# CODE-ALONG CHECKPOINT 2\n",
    "base_model = ?\n",
    "# freeze the weights\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the base model architecture\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`base_model` is our feature extractor. It converts a 160x160x3 image into a 5x5x1280 block of features.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(train_dataset))\n",
    "\n",
    "# CODE-ALONG CHECKPOINT 3\n",
    "feature_batch = ?\n",
    "print(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2 - Add a new classifier head on top that has the outputs we need for our task (car, truck, SUV, van)\n",
    "\n",
    "First, let's create a layer to average over the 5x5 spatial locations to create a single 1280 vector per image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a fully-connected layer of neurons as our outputs for our 4 classes. Each output neuron should be connected to each of the 1280 neurons in the previous layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE-ALONG CHECKPOINT 4 - Create a fully connected output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 3 - Connect this to our data pipeline to build a keras model ready for training\n",
    "Now let's connect all our parts together to build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE-ALONG CHECKPOINT 5 - Define our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "\n",
    "### Goal: Use an existing model to extract features from our images, and transfer that knowledge to our new task\n",
    "\n",
    "*Use our training data and a learning algorithm to find a good set of weights and biases that help our model solve the task*\n",
    "\n",
    "### Step by step runthrough:\n",
    "1. Compile our model for training, selecting relevant hyperparameters\n",
    "2. Train our model for 10 epochs\n",
    "3. Evaluate the performance of our new model on unseen data (the test set)\n",
    "\n",
    "#### Input: \n",
    "The model architecture we just constructed, our training and validation datasets\n",
    "#### Output\n",
    "A model that has learned how to perform our task\n",
    "\n",
    "## Quick concepts\n",
    "\n",
    "### Loss\n",
    "A measure of how bad a given prediction is. If the prediction is perfect, the loss is zero, otherwise we get larger losses. There are many loss functions to choose from, which one is most appropriate depends on your task.\n",
    "\n",
    "### Optimizer\n",
    "When we train a model, we want to find the weights and biases that **minimize the loss**. We can do that using variations on stocastic gradient descent.\n",
    "\n",
    "<img src=\"optimisation_demo.gif\" alt=\"optimisation\" width=\"700px\">\n",
    "\n",
    "[Source](https://github.com/lilipads/gradient_descent_viz)\n",
    "\n",
    "### Metrics\n",
    "Statistical measures or scores we can use to monitor and assess model performance. Again, we have many options to choose from here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1 - Compile our model for training, selecting relevant hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE-ALONG CHECKPOINT 6 - Compile our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE-ALONG CHECKPOINT 7 - Evaluate prior to training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2 - Train our model for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE-ALONG CHECKPOINT 8 - Train our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 3 - Evaluate the performance of our new model on unseen data (the test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve a batch of images from the test set\n",
    "image_batch, label_batch = test_dataset.as_numpy_iterator().next()\n",
    "predictions = model.predict_on_batch(image_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Car', 'SUV', 'Truck', 'Van']\n",
    "fig, axarr = plt.subplots(8, 4, figsize=(10,20))\n",
    "axes = axarr.flatten()\n",
    "for i, (image, prediction) in enumerate(zip(image_batch, predictions)):\n",
    "    ax = axes[i]\n",
    "    ax.imshow(image)\n",
    "    top_label = np.argmax(prediction)\n",
    "    ax.set_title(f'{class_names[top_label]}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Fine tuning Challenge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is your challenge.\n",
    "\n",
    "In feature extraction, we were training a small number of new layers on top of the existing MobileNetV2 base model, and we froze the weights of that base model so that they were not updated during training.\n",
    "\n",
    "One way to increase performance further is to \"fine-tune\" the model by training the weights of the top layers of the base model alongside our classification layer. \n",
    "\n",
    "This process should force the weights of the base model to move from defining generic feature maps to features more specific for our task.\n",
    "\n",
    "There are quite a few hyperparameters to change here -- how many layers to allow fine-tuning for, what learning rate to use, etc. We've set you up with a starting point, but play around and see how much you can improve performance with fine-tuning. We will have a prize for the best improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un-freeze the top layers of the model\n",
    "All you need to do is unfreeze the base_model and set the bottom layers to be un-trainable. Then, you should recompile the model (necessary for these changes to take effect), and resume training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "\n",
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 100\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable =  False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate/10),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "len(model.trainable_variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue training the model - if trained to convergence earlier this will improve your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_epochs = 10\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = model.fit(train_dataset,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=history.epoch[-1],\n",
    "                         validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc += history_fine.history['accuracy']\n",
    "val_acc += history_fine.history['val_accuracy']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "print('Test accuracy :', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve a batch of images from the test set\n",
    "image_batch, label_batch = test_dataset.as_numpy_iterator().next()\n",
    "predictions = model.predict_on_batch(image_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Car', 'SUV', 'Truck', 'Van']\n",
    "fig, axarr = plt.subplots(8, 4, figsize=(10,20))\n",
    "axes = axarr.flatten()\n",
    "for i, (image, prediction) in enumerate(zip(image_batch, predictions)):\n",
    "    ax = axes[i]\n",
    "    ax.imshow(image)\n",
    "    top_label = np.argmax(prediction)\n",
    "    score = prediction[top_label]\n",
    "    ax.set_title(f'{class_names[top_label]}, {score:.2f}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
